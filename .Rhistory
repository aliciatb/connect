colors <- configs$config$choropleth_map_config$category_wise_colors
indicator_settings <- fromJSON(colors)
indicator_settings %>%
select(super_category, customColorConfigs) %>%
unnest(customColorConfigs) %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
kableExtra::scroll_box(width = "1000px", height = "1200px")
library(dplyr)
library(jsonlite)
library(kableExtra)
library(knitr)
library(tidyr)
configs <- fromJSON("readiness_check.json")
colors <- configs$config$choropleth_map_config$category_wise_colors
indicator_settings <- fromJSON(colors)
View(indicator_settings)
indicator_settings %>%
select(super_category, customColorConfigs) %>%
unnest(customColorConfigs) %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
kableExtra::scroll_box(width = "1000px", height = "1200px")
library(pagedown)
install.packages("pagedown")
library(devtools)
remotes::install_github('rstudio/pagedown')
library(dplyr)
library(flexdashboard)
library(ggplot2)
library(httr)
library(jsonlite)
library(readr)
library(stringr)
library(tidyr)
datasets <- read_json("data/total_response.json")
View(datasets)
api_mode = F
if(api_mode == F){
datasets <- read_json("data/total_response.json")
}else{
datasets <- fromJSON(discoURL(),simplifyDataFrame = TRUE)
}
resource <- datasets$results$resource
View(datasets)
raw_data <- resource %>%
select(id,name,description,download_count,updatedAt,createdAt)
resource <- datasets$results$resource
datasets <- read_json("data/total_response.json")
resource <- datasets$results$resource
datasets
datasets <- read_json("data/total_response.json",simplifyDataFrame = TRUE)
resource <- datasets$results$resource
View(resource)
meta <- datasets$results$metadata
raw_data <- resource %>%
select(id,name,description,download_count,updatedAt,createdAt)
page_views <- resource$page_views
# link
link <- datasets$results$link
# domain
domain <- meta$domain
data <- cbind(domain, raw_data, page_views, link)
View(data)
View(raw_data)
View(resource)
raw_data <- resource %>%
select(id,name,description,attribution,updatedAt,createdAt,download_count)
# page view stats
page_views <- resource$page_views
# meta
meta <- datasets$results$metadata
# link
link <- datasets$results$link
# domain
domain <- meta$domain
data <- cbind(domain, raw_data, page_views, link)
# domain category
classification <- datasets$results$classification
categories - <- classification$categories
categories <- classification$categories
data <- cbind(domain, raw_data, page_views, link)
classification <- datasets$results$classification
data <- cbind(data, classification$domain_category, classification$categories) %>%
rename(category = `classification$domain_category`)
categories <- classification$categories
categories
data <- cbind(domain, raw_data, page_views, link, classification)
classification <- datasets$results$classification %L%
select(domain_category, domain_tags, domain_metadata)
classification <- datasets$results$classification %>%
select(domain_category, domain_tags, domain_metadata)
data <- cbind(domain, raw_data, page_views, link, classification)
raw_data <- resource %>%
select(id,name,description,attribution,updatedAt,createdAt,download_count)
# page view stats
page_views <- resource$page_views
# meta
meta <- datasets$results$metadata
# link
link <- datasets$results$link
# domain
domain <- meta$domain
# domain category
classification <- datasets$results$classification %>%
select(domain_category, domain_tags, domain_metadata)
data <- cbind(domain, raw_data, page_views, link, classification)
# set selected rank variable for plots to adjust y variable easily
if(input$rank == 'page_views_last_month'){
data <- data %>%
mutate(measure_value = page_views_last_month)
}
if(input$rank == 'page_views_last_week'){
data <- data %>%
mutate(measure_value = page_views_last_week)
}
if(input$rank == 'page_views_total'){
data <- data %>%
mutate(measure_value = page_views_total)
}
# capitalize x variables and replace NAs with 0 in download_count
data <- data %>%
mutate(download_count = replace_na(download_count, 0))
View(data)
data_view <- discoData()
data_view <- data
View(data_view)
ggplot(data_view) +
geom_bar(mapping = aes(x = domain, y = meta_score), na.rm = TRUE, stat = "identity")
View(datasets)
View(data)
View(data_view)
static_file <- 'data/total_response.json'
datasets <- read_json(static_file,simplifyDataFrame = TRUE)
resource <- datasets$results$resource
raw_data <- resource %>%
select(id,name,description,attribution,updatedAt,createdAt,download_count)
# page view stats
page_views <- resource$page_views
# meta
meta <- datasets$results$metadata
# link
link <- datasets$results$link
# domain
domain <- meta$domain
# domain category
classification <- datasets$results$classification %>%
select(domain_category, domain_tags, domain_metadata)
data <- cbind(domain, raw_data, page_views, link, classification)
View(data)
data <- data %>%
mutate(measure_value = page_views_last_week)
data <- data %>%
mutate(download_count = replace_na(download_count, 0)) %>%
mutate(has_description = ifelse(is.na(description), 0, 1 * input$description_weight)) %>%
mutate(has_category = ifelse(is.na(domain_category), 0, 1 * input$category_weight)) %>%
mutate(has_source = ifelse(is.na(attribution), 0, 1 * input$source_weight)) %>%
mutate(has_keywords = ifelse(is.na(domain_tags), 0, 1)) %>%
mutate(`Metadata Score` = (has_description + has_category + has_source + has_keywords)/4 * 100)
api_mode = F
data_view <- data %>%
select(measure_value, domain, `Metadata Score`)
group_by(domain) %>%
summarize(`Metadata Score` = mean(`Metadata Score`),
Value = sum(measure_value))
View(data)
View(data)
data$domain_metadata == list()
length(data$domain_metadata)
data$domain_metadata
data[20,]
data[20,18]
data[20,19]
length(data[20,18])
data[20,18] == NULL
is.na(data[20,18])
is.na.data.frame(data[20,18])
is.na.data.frame(data[21,18])
is.na.data.frame(data[,18])
is.null(data[20,18])
is.null(data[19,18])
is.na.data.frame(data[19,18])
is.na.data.frame(data[20,18])
data.frame((data[19,18])
fasd
type(data[20,18])
typeof(data[20,18])
typeof(data[19,18])
n_distinct(data[20,18])
n_distinct(data[19,18])
n_distinct(data[18,18])
is.pairlist(data[19,18])
is.pairlist(data[20,18])
data[20,18][0]
data[20,18][1]
data[20,18][2]
data[19,18][2]
data[19,18][1]
data[20,18][1]
data[20,18][1] == 'data frame with 0 columns and 0 rows'
length(data[20,18][1])
length(data[19,18][1])
length(data[19,18][1])$key
length(data[19,18][1])['key']
data[19,18][1]$key
data[19,18][1]['key']
data[19,18][1]
data[20,18][1]
typeof(data[20,18][1])
View(data)
View(data)
filter(data == 'data.seattle.gov')
library(tigris)
mi <- places("26", cb = TRUE)
plot(mi)
library(tidycensus)
library(dplyr)
mi_county_population <- get_acs(geography = "county", state = "MI", year = 2017, variables = "B01003_001E", geometry = FALSE)
mi_county_population
str_split("data.michigan.gov,future.michigan.gov",",")
library(dplyr)
library(flexdashboard)
library(ggplot2)
library(httr)
library(jsonlite)
library(readr)
library(shiny)
library(stringr)
library(tidyr)
require(wesanderson)
str_split("data.michigan.gov,future.michigan.gov",",")
str_split("data.michigan.gov,future.michigan.gov",",")[0]
str_split("data.michigan.gov,future.michigan.gov",",")(0)
primary_domain <- str_split("data.michigan.gov,future.michigan.gov",",",0)
primary_domain
primary_domain[0]
library(dplyr)
library(flexdashboard)
library(ggplot2)
library(httr)
library(jsonlite)
library(lubridate)
library(readr)
library(shiny)
library(stringr)
library(tidyr)
require(wesanderson)
datasets <- fromJSON('http://api.us.socrata.com/api/catalog/v1?published=true&only=dataset&limit=10000&order=page_views_last_month&domains=www.dallasopendata.com',simplifyDataFrame = TRUE)
resource <- datasets$results$resource
raw_data <- resource %>%
select(id,name,description,attribution,updatedAt,createdAt,download_count)
# page view stats/update/create
page_views <- resource$page_views
last_updated <- resource$updatedAt
created <- resource$createdAt
# meta
meta <- datasets$results$metadata
# link
link <- datasets$results$link
# domain
domain <- meta$domain
# domain category
classification <- datasets$results$classification %>%
select(domain_category, domain_tags, domain_metadata)
data <- cbind(domain, raw_data, page_views, last_updated, created, link, classification)
View(data)
'key' %in% data$domain_metadata
'key' %in% names(data$domain_metadata)
'key' %in% names(data[1]$domain_metadata)
data$domain_metadata
ata$domain_metadata[0]
data$domain_metadata[0]
'key' %in% names(data$domain_metadata[0])
key %in% names(data$domain_metadata[0])
is.na(data$domain_metadata[0])
is.na(data$domain_metadata[0])==1
match('key',data$domain_metadata)
match('key',data$domain_metadata[0])
is.element('key',data$domain_metadata)
data$has_custom <- is.element('key',data$domain_metadata)
View(data)
data$has_custom <- is.element('Addition',data$domain_metadata)
View(data)
typeof(data$domain_metadata)
data$has_custom <- is.vector(data$domain_metadata)
View(data)
data$has_custom <- is.list(data$domain_metadata)
View(data)
data$has_custom <- str(data$domain_metadata)
View(data)
data$has_custom <- str_replace(str(data$domain_metadata),'list()','')
data$has_custom <- unnest(data$domain_metadata)
library(tidyr)
data$has_custom <- unnest(data$domain_metadata)
data$has_custom <- nchar(data$domain_metadata)
View(data)
library(dplyr)
library(flexdashboard)
library(ggplot2)
library(httr)
library(jsonlite)
library(lubridate)
library(readr)
library(shiny)
library(stringr)
library(tidyr)
require(wesanderson)
api_mode = T
datasets <- fromJSON('http://api.us.socrata.com/api/catalog/v1?published=true&only=dataset&limit=10000&order=page_views_last_month&domains=data.seattle.gov',simplifyDataFrame = TRUE)
resource <- datasets$results$resource
raw_data <- resource %>%
select(id,name,description,attribution,updatedAt,createdAt,download_count)
# page view stats/update/create
page_views <- resource$page_views
last_updated <- resource$updatedAt
created <- resource$createdAt
meta <- datasets$results$metadata
# link
link <- datasets$results$link
# domain
domain <- meta$domain
# domain category
classification <- datasets$results$classification %>%
select(domain_category, domain_tags, domain_metadata)
data <- cbind(domain, raw_data, page_views, last_updated, created, link, classification)
data <- data %>%
mutate(measure_value = page_views_total)
data <- data %>%
mutate(download_count = replace_na(download_count, 0)) %>%
mutate(days_last_updated = (now() - ymd_hms(last_updated))/ddays(1))
View(data)
write_csv(data, "data/data_seattle_gov_response.csv")
View(data)
debug_data <- data %>%
select(id, name, domain)
write_csv(debug_data, "data/data_seattle_gov_response.csv")
View(data)
domains <- 'data.seattle.gov,data.mesaaz.gov,www.dallasopendata.com'
str_detect(domains, '.com$')
str_detect(domains, '.com$|.gov$')
str_detect(domains, '.com$|.gov$|.org$|.edu$')
library(readr)
library(readr)
ai <- read_csv('/users/alicia.brown/repos/connect/data/Public_Asset_Inventory_Final.csv')
ai.summary()
library(readr)
ai <- read_csv('/users/alicia.brown/repos/connect/data/Public_Asset_Inventory_Final.csv')
summary(ai)
View(ai)
library(lubridate)
now()
ai <- ai %>%
mutate(now = now(),
last_update =ymd_hms(last_update_date_data))
View(ai)
ai <- ai %>%
mutate(now = now(),
last_update =mdy_hms(last_update_date_data))
View(ai)
ai <- ai %>%
mutate(now = now(),
last_update = mdy_hms(last_update_date_data),
days_last_update = last_update - now)
View(ai)
ai <- ai %>%
mutate(now = now(),
last_update = mdy_hms(last_update_date_data),
days_last_update = now - last_update) %>%
order(days_last_update)
View(ai)
ai <- ai %>%
mutate(now = now(),
last_update = mdy_hms(last_update_date_data),
days_last_update = now - last_update)
View(ai)
ai <- ai %>%
mutate(now = now(),
last_update = mdy_hms(last_update_date_data),
days_last_update = now - last_update) %>%
arrange(desc(days_last_update))
View(ai)
median(ai$days_last_update)
ai <- ai %>%
mutate(now = now(),
last_update = mdy_hms(last_update_date_data),
days_last_update = (now - last_update)//ddays(1)) %>%
arrange(desc(days_last_update))
ai <- ai %>%
mutate(now = now(),
last_update = mdy_hms(last_update_date_data),
days_last_update = (now - last_update)/ddays(1)) %>%
arrange(desc(days_last_update))
View(ai)
median(ai$days_last_update)
mean(ai$days_last_update)
View(data)
median(data$days_last_updated)
mean(data$days_last_updated)
write_csv(data,'data_seattle.csv',na='')
qa <- data %>%
select(id,name,createdAt,updatedAt,last_updated)
View(qa)
qa <- data %>%
select(id,name,createdAt,updatedAt,days_last_updated)
View(qa)
qa <- data %>%
select(id,name,createdAt,updatedAt,now(),days_last_updated)
View(qa)
qa <- data %>%
select(id,name,createdAt,updatedAt,now,days_last_updated)
qa <- data %>%
mutate(today = now()) %>%
select(id,name,createdAt,updatedAt,today,days_last_updated)
qa <- data %>%
mutate(today = now()) %>%
select(id,name,createdAt,updatedAt,today,days_last_updated)
View(qa)
qa <- data %>%
mutate(today = now()) %>%
select(id,name,createdAt,updatedAt,today,days_last_updated) %>%
arrange(desc(days_last_updated))
View(qa)
View(data)
?dplyr
?`dplyr-package`
library(scales)
test_date = data['last_updated',1]
test_date
test <- data[1,1]
test
test <- data [,1]
test
test <- data [,1]
test
data$last_updated
test_date <- ymd_hms('2019-02-28T18:35:57.000Z')
format(test_date, "%Y-%m-%d")
# Libraries
library(flexdashboard)
library(tidyverse)
library(RSocrata)
library(readxl)
library(lubridate)
options(tz="US/Michigan")  # req for read.socrata()
knitr::opts_chunk$set(fig.width=16, fig.height=8)
setwd('~/repos/connect')
# Libraries
library(flexdashboard)
library(tidyverse)
library(RSocrata)
library(readxl)
library(lubridate)
options(tz="US/Michigan")  # req for read.socrata()
knitr::opts_chunk$set(fig.width=16, fig.height=8)
#### MI Economic Outcomes ####
# Unemployment, Labor Force, LFPR, U-6
econ_url <- "https://future.michigan.gov/Economy/LMISI-Monthly-Measures/v2fn-bucf"
econ <- read.socrata(econ_url)
#### Join MI Unemployment Data ####
# Unemployment Rates
unemp_url <- "https://data.michigan.gov/Economy/Unemployment-Rate/kamp-ngb9"
unemp <- read.socrata(unemp_url) %>%
# convert to tibble class
as_tibble() %>%
# remove unwanted rows
select(-highlights, -rank) %>%
# reformat rates as percentages
mutate(unemployment_rate_us = unemployment_rate_us * 100,
unemployment_rate_mi = unemployment_rate_mi * 100,
# reformat dates
date = as_date(parse_date_time(date, orders = "m-y"))) %>%
# rename columns
rename(MI = unemployment_rate_mi,
US = unemployment_rate_us)
#### FRED County Data ####
counties <- read_csv("../data/fredgraph.csv") %>%
rename(Wayne = MIWAYN3URN,
Kalamazoo = MIKALA7URN,
Saginaw = MISAGI5URN,
Ingham = MIINGH5URN)
counties <- read_csv("../data/fredgraph.csv")
counties <- read_csv("../data/fredgraph.csv")
#### MI Economic Outcomes ####
# Unemployment, Labor Force, LFPR, U-6
econ_url <- "https://future.michigan.gov/Economy/LMISI-Monthly-Measures/v2fn-bucf"
econ <- read.socrata(econ_url)
#### Join MI Unemployment Data ####
# Unemployment Rates
unemp_url <- "https://data.michigan.gov/Economy/Unemployment-Rate/kamp-ngb9"
unemp <- read.socrata(unemp_url) %>%
# convert to tibble class
as_tibble() %>%
# remove unwanted rows
select(-highlights, -rank) %>%
# reformat rates as percentages
mutate(unemployment_rate_us = unemployment_rate_us * 100,
unemployment_rate_mi = unemployment_rate_mi * 100,
# reformat dates
date = as_date(parse_date_time(date, orders = "m-y"))) %>%
# rename columns
rename(MI = unemployment_rate_mi,
US = unemployment_rate_us)
#### FRED County Data ####
counties <- read_csv("../data/fredgraph.csv") %>%
rename(Wayne = MIWAYN3URN,
Kalamazoo = MIKALA7URN,
Saginaw = MISAGI5URN,
Ingham = MIINGH5URN)
#### Join Unemployment Data ####
unemployment <- unemp %>%
# return matching rows from counties
left_join(counties, by = c("date" = "DATE"))
